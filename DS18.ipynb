{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a00eb26-d84f-4e22-b543-307758d18fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  20000 non-null  object\n",
      " 1   xbox    20000 non-null  int64 \n",
      " 2   ybox    20000 non-null  int64 \n",
      " 3   width   20000 non-null  int64 \n",
      " 4   height  20000 non-null  int64 \n",
      " 5   onpix   20000 non-null  int64 \n",
      " 6   xbar    20000 non-null  int64 \n",
      " 7   ybar    20000 non-null  int64 \n",
      " 8   x2bar   20000 non-null  int64 \n",
      " 9   y2bar   20000 non-null  int64 \n",
      " 10  xybar   20000 non-null  int64 \n",
      " 11  x2ybar  20000 non-null  int64 \n",
      " 12  xy2bar  20000 non-null  int64 \n",
      " 13  xedge   20000 non-null  int64 \n",
      " 14  xedgey  20000 non-null  int64 \n",
      " 15  yedge   20000 non-null  int64 \n",
      " 16  yedgex  20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "Dataset Summary:\n",
      "               xbox          ybox         width       height         onpix  \\\n",
      "count  20000.000000  20000.000000  20000.000000  20000.00000  20000.000000   \n",
      "mean       4.023550      7.035500      5.121850      5.37245      3.505850   \n",
      "std        1.913212      3.304555      2.014573      2.26139      2.190458   \n",
      "min        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
      "25%        3.000000      5.000000      4.000000      4.00000      2.000000   \n",
      "50%        4.000000      7.000000      5.000000      6.00000      3.000000   \n",
      "75%        5.000000      9.000000      6.000000      7.00000      5.000000   \n",
      "max       15.000000     15.000000     15.000000     15.00000     15.000000   \n",
      "\n",
      "               xbar          ybar         x2bar         y2bar         xybar  \\\n",
      "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
      "mean       6.897600      7.500450      4.628600      5.178650      8.282050   \n",
      "std        2.026035      2.325354      2.699968      2.380823      2.488475   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        6.000000      6.000000      3.000000      4.000000      7.000000   \n",
      "50%        7.000000      7.000000      4.000000      5.000000      8.000000   \n",
      "75%        8.000000      9.000000      6.000000      7.000000     10.000000   \n",
      "max       15.000000     15.000000     15.000000     15.000000     15.000000   \n",
      "\n",
      "            x2ybar        xy2bar         xedge        xedgey         yedge  \\\n",
      "count  20000.00000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
      "mean       6.45400      7.929000      3.046100      8.338850      3.691750   \n",
      "std        2.63107      2.080619      2.332541      1.546722      2.567073   \n",
      "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        5.00000      7.000000      1.000000      8.000000      2.000000   \n",
      "50%        6.00000      8.000000      3.000000      8.000000      3.000000   \n",
      "75%        8.00000      9.000000      4.000000      9.000000      5.000000   \n",
      "max       15.00000     15.000000     15.000000     15.000000     15.000000   \n",
      "\n",
      "            yedgex  \n",
      "count  20000.00000  \n",
      "mean       7.80120  \n",
      "std        1.61747  \n",
      "min        0.00000  \n",
      "25%        7.00000  \n",
      "50%        8.00000  \n",
      "75%        9.00000  \n",
      "max       15.00000  \n",
      "\n",
      "First Few Rows:\n",
      "  letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  xybar  \\\n",
      "0      T     2     8      3       5      1     8    13      0      6      6   \n",
      "1      I     5    12      3       7      2    10     5      5      4     13   \n",
      "2      D     4    11      6       8      6    10     6      2      6     10   \n",
      "3      N     7    11      6       6      3     5     9      4      6      4   \n",
      "4      G     2     1      3       1      1     8     6      6      6      6   \n",
      "\n",
      "   x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
      "0      10       8      0       8      0       8  \n",
      "1       3       9      2       8      4      10  \n",
      "2       3       7      3       7      3       9  \n",
      "3       4      10      6      10      2       8  \n",
      "4       5       9      1       7      5      10  \n",
      "\n",
      "Training Set Shape: (16000, 16)\n",
      "Test Set Shape: (4000, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('D:/Data science ass/18/Neural networks/Alphabets_data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['letter'])\n",
    "y = df['letter']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the splits\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f426d5-fe1a-4303-92ab-cdea5ba023ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Shape: (16000, 16)\n",
      "Test Set Shape: (4000, 16)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishabh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 - 2s - 3ms/step - accuracy: 0.5539 - loss: 1.6214 - val_accuracy: 0.7408 - val_loss: 0.9171\n",
      "Epoch 2/20\n",
      "500/500 - 0s - 729us/step - accuracy: 0.7756 - loss: 0.7828 - val_accuracy: 0.8138 - val_loss: 0.6626\n",
      "Epoch 3/20\n",
      "500/500 - 0s - 711us/step - accuracy: 0.8244 - loss: 0.6004 - val_accuracy: 0.8468 - val_loss: 0.5360\n",
      "Epoch 4/20\n",
      "500/500 - 0s - 700us/step - accuracy: 0.8522 - loss: 0.4943 - val_accuracy: 0.8643 - val_loss: 0.4601\n",
      "Epoch 5/20\n",
      "500/500 - 0s - 713us/step - accuracy: 0.8751 - loss: 0.4216 - val_accuracy: 0.8765 - val_loss: 0.4095\n",
      "Epoch 6/20\n",
      "500/500 - 0s - 737us/step - accuracy: 0.8885 - loss: 0.3673 - val_accuracy: 0.8888 - val_loss: 0.3655\n",
      "Epoch 7/20\n",
      "500/500 - 0s - 739us/step - accuracy: 0.9024 - loss: 0.3256 - val_accuracy: 0.8942 - val_loss: 0.3385\n",
      "Epoch 8/20\n",
      "500/500 - 0s - 725us/step - accuracy: 0.9126 - loss: 0.2922 - val_accuracy: 0.9043 - val_loss: 0.3052\n",
      "Epoch 9/20\n",
      "500/500 - 0s - 716us/step - accuracy: 0.9186 - loss: 0.2656 - val_accuracy: 0.9140 - val_loss: 0.2750\n",
      "Epoch 10/20\n",
      "500/500 - 0s - 758us/step - accuracy: 0.9264 - loss: 0.2431 - val_accuracy: 0.9195 - val_loss: 0.2644\n",
      "Epoch 11/20\n",
      "500/500 - 0s - 725us/step - accuracy: 0.9323 - loss: 0.2229 - val_accuracy: 0.9270 - val_loss: 0.2475\n",
      "Epoch 12/20\n",
      "500/500 - 0s - 725us/step - accuracy: 0.9362 - loss: 0.2095 - val_accuracy: 0.9265 - val_loss: 0.2402\n",
      "Epoch 13/20\n",
      "500/500 - 0s - 734us/step - accuracy: 0.9401 - loss: 0.1953 - val_accuracy: 0.9312 - val_loss: 0.2225\n",
      "Epoch 14/20\n",
      "500/500 - 0s - 739us/step - accuracy: 0.9451 - loss: 0.1808 - val_accuracy: 0.9337 - val_loss: 0.2130\n",
      "Epoch 15/20\n",
      "500/500 - 0s - 742us/step - accuracy: 0.9469 - loss: 0.1706 - val_accuracy: 0.9277 - val_loss: 0.2109\n",
      "Epoch 16/20\n",
      "500/500 - 0s - 712us/step - accuracy: 0.9489 - loss: 0.1615 - val_accuracy: 0.9348 - val_loss: 0.2016\n",
      "Epoch 17/20\n",
      "500/500 - 0s - 738us/step - accuracy: 0.9525 - loss: 0.1503 - val_accuracy: 0.9383 - val_loss: 0.1909\n",
      "Epoch 18/20\n",
      "500/500 - 0s - 729us/step - accuracy: 0.9543 - loss: 0.1443 - val_accuracy: 0.9410 - val_loss: 0.1851\n",
      "Epoch 19/20\n",
      "500/500 - 0s - 759us/step - accuracy: 0.9579 - loss: 0.1365 - val_accuracy: 0.9398 - val_loss: 0.1931\n",
      "Epoch 20/20\n",
      "500/500 - 0s - 717us/step - accuracy: 0.9612 - loss: 0.1280 - val_accuracy: 0.9348 - val_loss: 0.1883\n",
      "\n",
      "Test Loss: 0.18828000128269196\n",
      "Test Accuracy: 0.9347500205039978\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('D:/Data science ass/18/Neural networks/Alphabets_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['letter'])\n",
    "y = df['letter']\n",
    "\n",
    "# Encode the target variable as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the splits\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the ANN model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(16,)),  # First hidden layer\n",
    "    layers.Dense(64, activation='relu'),                    # Second hidden layer\n",
    "    layers.Dense(26, activation='softmax')                  # Output layer (26 classes for each letter)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\nTest Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5685d95-ebee-41af-a1ce-51b44758d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy:  0.937749981880188\n",
      "Best Parameters:  {'batch_size': 32, 'epochs': 20, 'optimizer': 'adam', 'init': 'glorot_uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the data as before\n",
    "# ... [Load data, encode labels, split into train/test sets, etc.]\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(optimizer='adam', init='glorot_uniform'):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', kernel_initializer=init, input_shape=(16,)),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer=init),\n",
    "        layers.Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters to try\n",
    "batch_sizes = [32, 64]\n",
    "epochs = [10, 20]\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "inits = ['glorot_uniform', 'normal']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Manual grid search\n",
    "for batch_size in batch_sizes:\n",
    "    for epoch in epochs:\n",
    "        for optimizer in optimizers:\n",
    "            for init in inits:\n",
    "                model = create_model(optimizer=optimizer, init=init)\n",
    "                model.fit(X_train, y_train, batch_size=batch_size, epochs=epoch, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'batch_size': batch_size,\n",
    "                        'epochs': epoch,\n",
    "                        'optimizer': optimizer,\n",
    "                        'init': init\n",
    "                    }\n",
    "\n",
    "print(\"Best Accuracy: \", best_accuracy)\n",
    "print(\"Best Parameters: \", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db8e9771-ac0f-4cf9-a248-0d9708dc9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.5754 - loss: 1.5963\n",
      "Epoch 2/20\n",
      "500/500 - 0s - 581us/step - accuracy: 0.7802 - loss: 0.7598\n",
      "Epoch 3/20\n",
      "500/500 - 0s - 577us/step - accuracy: 0.8274 - loss: 0.5851\n",
      "Epoch 4/20\n",
      "500/500 - 0s - 575us/step - accuracy: 0.8555 - loss: 0.4833\n",
      "Epoch 5/20\n",
      "500/500 - 0s - 595us/step - accuracy: 0.8796 - loss: 0.4095\n",
      "Epoch 6/20\n",
      "500/500 - 0s - 578us/step - accuracy: 0.8922 - loss: 0.3594\n",
      "Epoch 7/20\n",
      "500/500 - 0s - 557us/step - accuracy: 0.9042 - loss: 0.3170\n",
      "Epoch 8/20\n",
      "500/500 - 0s - 583us/step - accuracy: 0.9151 - loss: 0.2847\n",
      "Epoch 9/20\n",
      "500/500 - 0s - 580us/step - accuracy: 0.9224 - loss: 0.2587\n",
      "Epoch 10/20\n",
      "500/500 - 0s - 569us/step - accuracy: 0.9276 - loss: 0.2366\n",
      "Epoch 11/20\n",
      "500/500 - 0s - 575us/step - accuracy: 0.9342 - loss: 0.2164\n",
      "Epoch 12/20\n",
      "500/500 - 0s - 618us/step - accuracy: 0.9374 - loss: 0.2018\n",
      "Epoch 13/20\n",
      "500/500 - 0s - 559us/step - accuracy: 0.9418 - loss: 0.1870\n",
      "Epoch 14/20\n",
      "500/500 - 0s - 572us/step - accuracy: 0.9455 - loss: 0.1738\n",
      "Epoch 15/20\n",
      "500/500 - 0s - 585us/step - accuracy: 0.9483 - loss: 0.1631\n",
      "Epoch 16/20\n",
      "500/500 - 0s - 577us/step - accuracy: 0.9521 - loss: 0.1526\n",
      "Epoch 17/20\n",
      "500/500 - 0s - 578us/step - accuracy: 0.9557 - loss: 0.1433\n",
      "Epoch 18/20\n",
      "500/500 - 0s - 568us/step - accuracy: 0.9571 - loss: 0.1337\n",
      "Epoch 19/20\n",
      "500/500 - 0s - 567us/step - accuracy: 0.9601 - loss: 0.1291\n",
      "Epoch 20/20\n",
      "500/500 - 0s - 566us/step - accuracy: 0.9619 - loss: 0.1202\n",
      "\n",
      "Best Model Test Accuracy: 0.9452499747276306\n"
     ]
    }
   ],
   "source": [
    "# After running the manual grid search and finding the best parameters\n",
    "best_params = {\n",
    "    'batch_size': 32,  \n",
    "    'epochs': 20,      \n",
    "    'optimizer': 'adam',  \n",
    "    'init': 'glorot_uniform'  \n",
    "}\n",
    "\n",
    "# Recreate the best model with the optimal hyperparameters\n",
    "best_model = create_model(optimizer=best_params['optimizer'], init=best_params['init'])\n",
    "\n",
    "# Train the model using the entire training set with the best hyperparameters\n",
    "best_model.fit(X_train, y_train, batch_size=best_params['batch_size'], epochs=best_params['epochs'], verbose=2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\nBest Model Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c339676-1914-4e67-b922-97c29d16aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
